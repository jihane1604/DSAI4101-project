{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a527bede-d3c3-4353-af92-b5ed2d2017ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to sys.path: C:\\Users\\sadek\\OneDrive\\Desktop\\DSAI4101-project\n",
      "Current working directory: C:\\Users\\sadek\\OneDrive\\Desktop\\DSAI4101-project\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up one level: from notebooks/ → project root\n",
    "project_root = os.path.abspath(\"..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Added to sys.path:\", project_root)\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c3d164-10a8-45a6-8ab2-ef7b6013e013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "\n",
    "from src.b_models_impl import MyEmbeddingClient  # uses your new SimpleCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7654120a-c83a-4030-aed2-85b300ead257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n"
     ]
    }
   ],
   "source": [
    "emb_client = MyEmbeddingClient(\n",
    "    model_path=\"../models/classifier/simple_cnn.pth\",\n",
    "    classes_path=\"../models/classifier/classes.json\"\n",
    ")\n",
    "\n",
    "device = emb_client.device\n",
    "model = emb_client.model\n",
    "model.eval()\n",
    "\n",
    "print(\"Device:\", device)\n",
    "print(\"Classes:\", emb_client.idx_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "509afbcf-1b00-4172-bd0e-74ded0901802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known embeddings shape: (1767, 256)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) Load TRAIN data (normal classes)\n",
    "train_ds = datasets.ImageFolder(\n",
    "    root=\"../data/split/train\",        # your 6 normal classes\n",
    "    transform=emb_client.transform    # use EXACT same transform as classifier\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# 2) Extract embeddings\n",
    "all_embs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        embs = model.forward_features(imgs)   # (B, 256) with new model\n",
    "        all_embs.append(embs.cpu().numpy())\n",
    "\n",
    "X_known = np.concatenate(all_embs, axis=0)\n",
    "print(\"Known embeddings shape:\", X_known.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b87f2e9-e0e1-4db5-9a54-62277f84622b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly embeddings shape: (75, 256)\n"
     ]
    }
   ],
   "source": [
    "# Anomaly dataset (e.g. images in ../data/rare_classes/anomaly)\n",
    "anom_ds = datasets.ImageFolder(\n",
    "    root=\"../data/rare_classes\",\n",
    "    transform=emb_client.transform\n",
    ")\n",
    "\n",
    "anom_loader = DataLoader(anom_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "all_anom = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, _ in anom_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        embs = model.forward_features(imgs)\n",
    "        all_anom.append(embs.cpu().numpy())\n",
    "\n",
    "X_anom = np.concatenate(all_anom, axis=0)\n",
    "print(\"Anomaly embeddings shape:\", X_anom.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "889640a4-ffcc-4d57-b27b-7eb540b46a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known score range : -0.07430385966504749 → 0.1138009755979244\n",
      "Anomaly score range: -0.037781342821330166 → 0.09939493102825198\n",
      "\n",
      "Percentile | TP  FN  FP  TN\n",
      "---------------------------------\n",
      "        1 |  0  75   18  1749\n",
      "        2 |  1  74   36  1731\n",
      "        5 |  5  70   89  1678\n",
      "       10 | 11  64  177  1590\n",
      "       15 | 17  58  265  1502\n",
      "       20 | 19  56  354  1413\n",
      "       25 | 26  49  442  1325\n",
      "       30 | 31  44  530  1237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "iso.fit(X_known)\n",
    "\n",
    "scores_known = iso.decision_function(X_known)\n",
    "scores_anom  = iso.decision_function(X_anom)\n",
    "\n",
    "print(\"Known score range :\", scores_known.min(), \"→\", scores_known.max())\n",
    "print(\"Anomaly score range:\", scores_anom.min(), \"→\", scores_anom.max())\n",
    "\n",
    "# Try different thresholds (percentiles of NORMAL scores)\n",
    "percentiles = [1, 2, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "print(\"\\nPercentile | TP  FN  FP  TN\")\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "for p in percentiles:\n",
    "    thr = np.percentile(scores_known, p)  # p% of normal data will be below thr\n",
    "\n",
    "    pred_known = scores_known < thr   # below thr → anomaly\n",
    "    pred_anom  = scores_anom  < thr\n",
    "\n",
    "    tp = pred_anom.sum()\n",
    "    fn = len(pred_anom) - tp\n",
    "    fp = pred_known.sum()\n",
    "    tn = len(pred_known) - fp\n",
    "\n",
    "    print(f\"{p:9d} | {tp:2d}  {fn:2d}  {fp:3d}  {tn:4d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ee465b-287d-41b1-85d3-09f86e5c8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dim: 256\n",
      "PCA dim     : 10\n",
      "Score ranges:\n",
      "  Test normals: -0.09953923975420187 → 0.14330614374589679\n",
      "  Anomalies   : -0.03390019545616607 → 0.12567741436099822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "# 1) Fit PCA on TRAIN normals only\n",
    "pca = PCA(n_components=0.95, random_state=42)  # keep 95% variance\n",
    "pca.fit(X_known_train)\n",
    "\n",
    "print(\"Original dim:\", X_known_train.shape[1])\n",
    "print(\"PCA dim     :\", pca.n_components_)\n",
    "\n",
    "# 2) Transform train normals, test normals, anomalies\n",
    "Z_known_train = pca.transform(X_known_train)\n",
    "Z_known_test  = pca.transform(X_known_test)\n",
    "Z_anom        = pca.transform(X_anom)\n",
    "\n",
    "# 3) Train IsolationForest on PCA features\n",
    "iso_pca = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "iso_pca.fit(Z_known_train)\n",
    "\n",
    "scores_train_pca = iso_pca.decision_function(Z_known_train)\n",
    "scores_test_pca  = iso_pca.decision_function(Z_known_test)\n",
    "scores_anom_pca  = iso_pca.decision_function(Z_anom)\n",
    "\n",
    "print(\"Score ranges:\")\n",
    "print(\"  Test normals:\", scores_test_pca.min(), \"→\", scores_test_pca.max())\n",
    "print(\"  Anomalies   :\", scores_anom_pca.min(), \"→\", scores_anom_pca.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4d49364-b0af-459c-b925-68daac5b0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[IF+PCA] Percentile | TP  FN  FP  TN\n",
      "----------------------------------------\n",
      "        1 |  0  75    5   526\n",
      "        2 |  1  74   15   516\n",
      "        5 |  4  71   31   500\n",
      "       10 |  4  71   48   483\n",
      "       15 | 13  62   77   454\n",
      "       20 | 14  61  102   429\n",
      "       25 | 24  51  127   404\n",
      "       30 | 33  42  151   380\n"
     ]
    }
   ],
   "source": [
    "percentiles = [1, 2, 5, 10, 15, 20, 25, 30]\n",
    "\n",
    "print(\"\\n[IF+PCA] Percentile | TP  FN  FP  TN\")\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "for p in percentiles:\n",
    "    thr = np.percentile(scores_train_pca, p)  # based on TRAIN normals\n",
    "\n",
    "    pred_normals = scores_test_pca < thr\n",
    "    pred_anom    = scores_anom_pca < thr\n",
    "\n",
    "    tp = pred_anom.sum()\n",
    "    fn = len(pred_anom) - tp\n",
    "    fp = pred_normals.sum()\n",
    "    tn = len(pred_normals) - fp\n",
    "\n",
    "    print(f\"{p:9d} | {tp:2d}  {fn:2d}  {fp:3d}  {tn:4d}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
